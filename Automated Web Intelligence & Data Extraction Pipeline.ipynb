{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0ETtbz4KRZTa7YSYpp9YV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ceSJpIJREssR","executionInfo":{"status":"ok","timestamp":1767933986449,"user_tz":-330,"elapsed":11074,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import nltk\n","import requests\n","from bs4 import BeautifulSoup\n","from pathlib import Path\n","from nltk.tokenize import word_tokenize, sent_tokenize"]},{"cell_type":"code","source":["nltk.download('punkt', quiet=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytMJygN3H7nj","executionInfo":{"status":"ok","timestamp":1767934843471,"user_tz":-330,"elapsed":18,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}},"outputId":"49dd74c9-5d47-4703-b051-54bcad550511"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["CONFIGURATION"],"metadata":{"id":"8syRKNS1FCah"}},{"cell_type":"code","source":["INPUT_FILE = 'lead_sources.xlsx'\n","OUTPUT_FOLDER = \"extracted_data_logs\"\n","OUTPUT_EXCEL_PATH = \"Structured_Lead_Data.xlsx\"\n","HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}"],"metadata":{"id":"gBHwC9HyE3U5","executionInfo":{"status":"ok","timestamp":1767934057410,"user_tz":-330,"elapsed":6,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["HELPER FUNCTIONS"],"metadata":{"id":"U3Qt1xdfFRFK"}},{"cell_type":"code","source":["def count_syllables(word):\n","    \"\"\"Counts syllables for Fog Index calculation.\"\"\"\n","    word = word.lower()\n","    vowels = \"aeiouy\"\n","    count = 0\n","    if not word: return 0\n","    if word[0] in vowels: count += 1\n","    for i in range(1, len(word)):\n","        if word[i] in vowels and word[i - 1] not in vowels:\n","            count += 1\n","    if word.endswith('e'): count -= 1\n","    if word.endswith(('es', 'ed')): count -= 1\n","    return max(1, count)"],"metadata":{"id":"krCJxb0UE3V6","executionInfo":{"status":"ok","timestamp":1767934093252,"user_tz":-330,"elapsed":4,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def calculate_nlp_metrics(text):\n","    \"\"\"Calculates readability and complexity metrics.\"\"\"\n","    sentences = sent_tokenize(text)\n","    words = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n","\n","    num_sentences = len(sentences) if len(sentences) > 0 else 1\n","    num_words = len(words) if len(words) > 0 else 1\n","\n","    # Complex words (> 2 syllables)\n","    complex_words = [w for w in words if count_syllables(w) > 2]\n","    num_complex = len(complex_words)\n","\n","    avg_sent_len = num_words / num_sentences\n","    pct_complex = (num_complex / num_words) * 100\n","    fog_index = 0.4 * (avg_sent_len + pct_complex)\n","\n","    return {\n","        'Word_Count': num_words,\n","        'Avg_Sentence_Length': round(avg_sent_len, 2),\n","        'Complex_Word_Count': num_complex,\n","        'Fog_Index': round(fog_index, 2),\n","        'Avg_Word_Length': round(sum(len(w) for w in words) / num_words, 2) }"],"metadata":{"id":"KI-8xltpE3av","executionInfo":{"status":"ok","timestamp":1767934170027,"user_tz":-330,"elapsed":12,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def scrape_url(url):\n","    \"\"\"Robust scraper for extracting title and body text.\"\"\"\n","    try:\n","        response = requests.get(url, headers=HEADERS, timeout=15)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","\n","        # Generic selectors for broad compatibility\n","        title = soup.find('h1').get_text().strip() if soup.find('h1') else \"No Title Found\"\n","        paragraphs = soup.find_all('p')\n","        body_text = \"\\n\".join([p.get_text().strip() for p in paragraphs if p.get_text()])\n","\n","        return f\"{title}\\n\\n{body_text}\", title\n","    except Exception as e:\n","        return f\"Error: {str(e)}\", \"\""],"metadata":{"id":"aLpmcGfrE3b2","executionInfo":{"status":"ok","timestamp":1767934195827,"user_tz":-330,"elapsed":22,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["MAIN EXECUTION PIPELINE"],"metadata":{"id":"r8tx5JxZF16J"}},{"cell_type":"code","source":["def run_pipeline():\n","    # Loading Input (With Fallback for Demo)\n","    try:\n","        df_input = pd.read_excel(INPUT_FILE)\n","        tasks = df_input.to_dict('records')\n","        print(f\"Loaded {len(tasks)} URLs from {INPUT_FILE}\")\n","    except:\n","        print(\"Input file not found. Running in DEMO MODE with public URLs...\")\n","        tasks = [\n","            {'URL_ID': 'DEMO_01', 'URL': 'https://en.wikipedia.org/wiki/Data_science'},\n","            {'URL_ID': 'DEMO_02', 'URL': 'https://www.bbc.com/news/business'} ]\n","\n","    results = []\n","    Path(OUTPUT_FOLDER).mkdir(exist_ok=True)\n","\n","    for item in tasks:\n","        url = item['URL']\n","        uid = item['URL_ID']\n","        print(f\"Processing {uid}...\")\n","\n","        full_text, title = scrape_url(url)\n","\n","        if not full_text.startswith(\"Error\"):\n","            # Saving Raw Text Log\n","            with open(f\"{OUTPUT_FOLDER}/{uid}.txt\", \"w\", encoding=\"utf-8\") as f:\n","                f.write(full_text)\n","\n","            #  Analysis\n","            metrics = calculate_nlp_metrics(full_text)\n","\n","            #  CRM-Ready Row\n","            row = {\n","                'Lead_ID': uid,\n","                'Source_URL': url,\n","                'Title': title,\n","                'Status': 'Success'\n","            }\n","            row.update(metrics)\n","            results.append(row)\n","        else:\n","            results.append({'Lead_ID': uid, 'Source_URL': url, 'Status': 'Failed'})\n","\n","    return results\n"],"metadata":{"id":"V0WW3m1nE3gv","executionInfo":{"status":"ok","timestamp":1767934609679,"user_tz":-330,"elapsed":2,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Export to Excel\n","if __name__ == \"__main__\":\n","    results = run_pipeline() # Capture the returned results\n","    df_output = pd.DataFrame(results)\n","    df_output.to_excel(OUTPUT_EXCEL_PATH, index=False)\n","    print(f\"\\nSUCCESS! Results saved to {OUTPUT_EXCEL_PATH}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmLohShJE3h-","executionInfo":{"status":"ok","timestamp":1767934664940,"user_tz":-330,"elapsed":884,"user":{"displayName":"Hanusha Palangthod","userId":"02112394131115604558"}},"outputId":"884a8cdb-fc94-4d18-e53b-213ec66942a0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Input file not found. Running in DEMO MODE with public URLs...\n","Processing DEMO_01...\n","Processing DEMO_02...\n","\n","SUCCESS! Results saved to Structured_Lead_Data.xlsx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XUs_QX4_E3mj"},"execution_count":null,"outputs":[]}]}